{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/fashion/txt/fashion.dedup.txt'\n",
    "\n",
    "GLOVE_6B_VOCAB_PATH = '/data/models/glove.6B.vocab'\n",
    "GLOVE_6B_PATH = '/data/models/glove.6B.300d.txt'\n",
    "\n",
    "GLOVE_840B_VOCAB_PATH = '/data/models/glove.840B.vocab'\n",
    "GLOVE_840B_PATH = '/data/models/glove.840B.300d.txt'\n",
    "\n",
    "BLACK_LIST = string.punctuation.replace('%', '').replace('-','') + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(text, black_list = BLACK_LIST, vocab=None, lowercase =  True, tokenize = False):\n",
    "    if black_list:\n",
    "        text = text.translate(None, BLACK_LIST)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if vocab:\n",
    "        text = ' '.join([word for word in text.split() if word in vocab])\n",
    "    if tokenize:\n",
    "        return text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "\n",
    "def load_and_process(data_path, num_words, maxlen):\n",
    "    with open(data_path, 'rt') as f:\n",
    "        classes, texts =  zip(*[line.split(\" \", 1) for line in f.readlines()])\n",
    "        \n",
    "        # class preprocessing\n",
    "        classes = [cls[9:] for cls in classes]\n",
    "        class_to_id = { \n",
    "            key: index for (index, (key, value)) in enumerate(Counter(classes).most_common())\n",
    "        }\n",
    "        ids = to_categorical([class_to_id[cls] for cls in classes])\n",
    "    \n",
    "    # Setting up keras tokenzer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    logger.debug('Found %s unique tokens', len(word_index))\n",
    "\n",
    "    # Padding data\n",
    "    data = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    logger.debug('Shape of data tensor: %s', data.shape)\n",
    "    logger.debug('Shape of label tensor: %s', ids.shape)\n",
    "\n",
    "    return data, ids, tokenizer\n",
    "\n",
    "\n",
    "def load_glove_embeddings(embedding_path, word_index, max_sequence, trainable=True):\n",
    "    '''\n",
    "    Loads Glove word vectors\n",
    "    Arguments:\n",
    "        embedding_path  - path to GloVe word embeddings\n",
    "        word_index      - dictionary mapping words to their rank\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # create dictionary with embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(\" \", 1)\n",
    "            coefs = np.asarray(coefs.split(), dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    logger.debug('Found %s word vectors with shape', len(embeddings_index))\n",
    "\n",
    "    # for convenience\n",
    "    nrows, ncols = len(word_index) + 1, coefs.shape[0]\n",
    "    logger.debug(\"rows %s, columns %s\", nrows, ncols)\n",
    "\n",
    "    # words not found in embedding index will be all-zeros\n",
    "    embedding_matrix = np.zeros((nrows, ncols))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(nrows,\n",
    "                                ncols,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence,\n",
    "                                \n",
    "                                trainable=trainable)\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def train_val_split(data, labels, split_ratio, seed=0):\n",
    "    '''\n",
    "    Splits data and lables into training and validation set\n",
    "    '''\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # shuffle indices\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(split_ratio * data.shape[0])\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS = 7000\n",
    "MAX_SENT_LENGTH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data, labels, tokenizer = load_and_process(DATA_PATH, MAX_WORDS, MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split test, val\n",
    "x_train, y_train, x_val, y_val = train_val_split(data, labels, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer = load_glove_embeddings(GLOVE_6B_PATH, tokenizer.word_index, MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "nb_classes = labels.shape[1]\n",
    "weight_decay = 0.01\n",
    "weight_decay = 128\n",
    "filter_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# add dropout\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# add l2 regularization\n",
    "model.add(Dense(1024, name=\"embedding\", activation='relu', kernel_regularizer=l2(.01)))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Setup optimizer\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evalute model on validation data\n",
    "logger.info(\"Train data\")\n",
    "model.evaluate(x_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evalute model on validation data\n",
    "logger.info(\"Validation data\")\n",
    "model.evaluate(x_val, y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the exported input tensor (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ = sess.graph.get_tensor_by_name('embedding_1_input:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = sess.graph.get_tensor_by_name(\"embedding/Relu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export_path = '/data/models/cnn-word-fashion/' # where to save the exported graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# all new operations will be in test mode from now on (dropout, etc.)\n",
    "K.set_learning_phase(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# serialize the model and get its weights, for quick re-building\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "\n",
    "# re-build a model where the learning phase is now hard-coded to 0\n",
    "production_model = Sequential.from_config(config)\n",
    "production_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### export tokenizer to ensure consistency between training and production\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(export_path, 'tokenizer.pickle'), 'wb') as f_:\n",
    "    pickle.dump(tokenizer, f_, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
