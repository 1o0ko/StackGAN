{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "from keras.layers import Dense, Flatten, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Globals setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/fashion/txt/fashion.dedup.txt'\n",
    "\n",
    "GLOVE_6B_VOCAB_PATH = '/data/models/glove.6B.vocab'\n",
    "GLOVE_6B_PATH = '/data/models/glove.6B.300d.txt'\n",
    "\n",
    "GLOVE_840B_VOCAB_PATH = '/data/models/glove.840B.vocab'\n",
    "GLOVE_840B_PATH = '/data/models/glove.840B.300d.txt'\n",
    "\n",
    "BLACK_LIST = string.punctuation.replace('%', '').replace('-','') + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text, black_list = BLACK_LIST, vocab=None, lowercase =  True, tokenize = False):\n",
    "    if black_list:\n",
    "        text = text.translate(None, BLACK_LIST)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if vocab:\n",
    "        text = ' '.join([word for word in text.split() if word in vocab])\n",
    "    if tokenize:\n",
    "        return text.split()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Embedding\n",
    "\n",
    "def load_and_process(data_path, num_words, maxlen):\n",
    "    with open(data_path, 'rt') as f:\n",
    "        classes, texts =  zip(*[line.split(\" \", 1) for line in f.readlines()])\n",
    "        \n",
    "        # class preprocessing\n",
    "        classes = [cls[9:] for cls in classes]\n",
    "        class_to_id = { \n",
    "            key: index for (index, (key, value)) in enumerate(Counter(classes).most_common())\n",
    "        }\n",
    "        ids = to_categorical([class_to_id[cls] for cls in classes])\n",
    "    \n",
    "    # Setting up keras tokenzer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    logger.debug('Found %s unique tokens', len(word_index))\n",
    "\n",
    "    # Padding data\n",
    "    data = pad_sequences(sequences, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "    logger.debug('Shape of data tensor: %s', data.shape)\n",
    "    logger.debug('Shape of label tensor: %s', ids.shape)\n",
    "\n",
    "    return data, ids, tokenizer\n",
    "\n",
    "\n",
    "def load_glove_embeddings(embedding_path, word_index, max_sequence, trainable=True):\n",
    "    '''\n",
    "    Loads Glove word vectors\n",
    "    Arguments:\n",
    "        embedding_path  - path to GloVe word embeddings\n",
    "        word_index      - dictionary mapping words to their rank\n",
    "    '''\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    # create dictionary with embeddings\n",
    "    embeddings_index = {}\n",
    "    with open(embedding_path) as f:\n",
    "        for line in f:\n",
    "            word, coefs = line.split(\" \", 1)\n",
    "            coefs = np.asarray(coefs.split(), dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "\n",
    "    logger.debug('Found %s word vectors with shape', len(embeddings_index))\n",
    "\n",
    "    # for convenience\n",
    "    nrows, ncols = len(word_index) + 1, coefs.shape[0]\n",
    "    logger.debug(\"rows %s, columns %s\", nrows, ncols)\n",
    "\n",
    "    # words not found in embedding index will be all-zeros\n",
    "    embedding_matrix = np.zeros((nrows, ncols))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    embedding_layer = Embedding(nrows,\n",
    "                                ncols,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=max_sequence,\n",
    "                                \n",
    "                                trainable=trainable)\n",
    "    return embedding_layer\n",
    "\n",
    "\n",
    "def train_val_split(data, labels, split_ratio, seed=0):\n",
    "    '''\n",
    "    Splits data and lables into training and validation set\n",
    "    '''\n",
    "    # set seed\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # shuffle indices\n",
    "    indices = np.arange(data.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    data = data[indices]\n",
    "    labels = labels[indices]\n",
    "    nb_validation_samples = int(split_ratio * data.shape[0])\n",
    "\n",
    "    x_train = data[:-nb_validation_samples]\n",
    "    y_train = labels[:-nb_validation_samples]\n",
    "\n",
    "    x_val = data[-nb_validation_samples:]\n",
    "    y_val = labels[-nb_validation_samples:]\n",
    "\n",
    "    return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 7000\n",
    "MAX_SENT_LENGTH = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels, tokenizer = load_and_process(DATA_PATH, MAX_WORDS, MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test, val\n",
    "x_train, y_train, x_val, y_val = train_val_split(data, labels, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = load_glove_embeddings(GLOVE_6B_PATH, tokenizer.word_index, MAX_SENT_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.2\n",
    "nb_classes = labels.shape[1]\n",
    "weight_decay = 0.01\n",
    "weight_decay = 128\n",
    "filter_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(MaxPooling1D(3))\n",
    "model.add(Flatten())\n",
    "\n",
    "# add dropout\n",
    "model.add(Dropout(dropout_rate))\n",
    "\n",
    "# add l2 regularization\n",
    "model.add(Dense(1024, name=\"embedding\", activation='relu', kernel_regularizer=l2(.01)))\n",
    "model.add(Dense(nb_classes, activation='softmax'))\n",
    "\n",
    "# Setup optimizer\n",
    "adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=128,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute model on validation data\n",
    "logger.info(\"Train data\")\n",
    "model.evaluate(x_train, y_train, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalute model on validation data\n",
    "logger.info(\"Validation data\")\n",
    "model.evaluate(x_val, y_val, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = K.get_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the exported input tensor (placeholder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = sess.graph.get_tensor_by_name('embedding_1_input:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = sess.graph.get_tensor_by_name(\"embedding/Relu:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_path = '/data/models/cnn-word-fashion/' # where to save the exported graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# all new operations will be in test mode from now on (dropout, etc.)\n",
    "K.set_learning_phase(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model and get its weights, for quick re-building\n",
    "config = model.get_config()\n",
    "weights = model.get_weights()\n",
    "\n",
    "# re-build a model where the learning phase is now hard-coded to 0\n",
    "production_model = Sequential.from_config(config)\n",
    "production_model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### export tokenizer to ensure consistency between training and production\n",
    "import pickle\n",
    "\n",
    "with open(os.path.join(export_path, 'tokenizer.pickle'), 'wb') as f_:\n",
    "    pickle.dump(tokenizer, f_, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Tensorflow part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tensorflow.python.saved_model.builder import SavedModelBuilder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import  build_signature_def, predict_signature_def\n",
    "from tensorflow.contrib.session_bundle import exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver() \n",
    "builder = SavedModelBuilder(export_dir=export_path)\n",
    "\n",
    "signature = predict_signature_def(\n",
    "    inputs={'images': production_model.input},\n",
    "    outputs={'scores': production_model.output})\n",
    "\n",
    "sess = K.get_session()\n",
    "builder.add_meta_graph_and_variables(\n",
    "    sess=sess,\n",
    "    tags=[tag_constants.SERVING],\n",
    "    signature_def_map={'predict': signature})\n",
    "\n",
    "builder.save()\n",
    "saver.save(sess, os.path.join(export_path, 'model.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Char-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH, 'rt') as f:\n",
    "    classes, texts =  zip(*[line.split(\" \", 1) for line in f.readlines()])\n",
    "    classes = [cls[9:] for cls in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "labels, values = zip(*Counter(classes).most_common())\n",
    "\n",
    "# get the EDF\n",
    "perc = [0.9, 0.95, 0.99]\n",
    "cumsum = np.cumsum(values) / float(np.sum(values))\n",
    "idxs = [np.argmax(cumsum >= idx) for idx in perc]\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(20, 2))\n",
    "plt.title(\"Class frequency (%i classes)\" % len(values))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes + width * 0.5, labels)\n",
    "plt.xticks(rotation=90)\n",
    "for idx, text in zip(idxs, perc):\n",
    "    plt.axvline(x=idx, color='r')\n",
    "    plt.text(idx+1, 3500, \"%0.2f at %i\" % (text, idx) , color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens (GloVe comparision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_fasion = Counter(itertools.chain(*[normalize(text, tokenize=True) for text in texts]))\n",
    "top_words = [k for k,v in vocab_fasion.most_common()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %i elements in the dataset\" % len(vocab_fasion))\n",
    "vocab_fasion.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Distribution of sentence length\")\n",
    "plt.hist(lengths, bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sent = [text for text in texts if len(text) >= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_sent[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GLOVE_6B_VOCAB_PATH, 'rt') as f:\n",
    "    vocab_6b = set([line[:-1] for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(GLOVE_840B_VOCAB_PATH, 'rt') as f:\n",
    "    vocab_840b = set([line[:-1] for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_from_6B = Counter({k: vocab_fasion[k] for k in vocab_fasion if k not in vocab_6b})\n",
    "missing_from_840B = Counter({k: vocab_fasion[k] for k in vocab_fasion if k not in vocab_840b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %i elements missing from 840b out of %i\" % (len(missing_from_6B), len(vocab_fasion)))\n",
    "for k, v in missing_from_6B.most_common(10):\n",
    "    print(\"- %s with %i occurances, %i most common word\" % (k, missing_from_6B[k], top_words.index(k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %i elements missing from 840b out of %i\" % (len(missing_from_840B), len(vocab_fasion)))\n",
    "for k, v in missing_from_840B.most_common(10):\n",
    "    print(\"- %s with %i occurances, %i most common word\" % (k, v, top_words.index(k)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
