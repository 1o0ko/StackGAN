{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from collections import namedtuple\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "\n",
    "\n",
    "BATH_SIZE = 100\n",
    "IMG_SIZE = 256\n",
    "\n",
    "# Img resizing stuff\n",
    "LR_HR_RATIO = 4\n",
    "BIG_SIZE = int(IMG_SIZE * 76 / 64)\n",
    "SMALL_SIZE = int(BIG_SIZE / LR_HR_RATIO)\n",
    "\n",
    "DATA_TEMPLATE =             '/fashion/ssense_%i_%i.h5'\n",
    "LANGUAGE_MODEL_FILE =       '/models/glove/glove.6B.300d.txt'\n",
    "LANGUAGE_MODEL_VOCABULARY = '/models/glove/glove.6B.vocab'\n",
    "\n",
    "FASTTEXT_DATA =             '/data/fashion/txt/fashion.dedup.txt'\n",
    "FASTTEXT_DATA_TRAIN =       '/data/fashion/txt/fashion-train.txt'\n",
    "FASTTEXT_DATA_CLEAN =       '/data/fashion/txt/fashion-clean.txt'\n",
    "\n",
    "OUTPUT =                    '/data/classes_and_texts.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_set = H5PYDataset(DATA_TEMPLATE % (IMG_SIZE, IMG_SIZE), which_sets=('all',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def list_keys(data_set): \n",
    "    return [k.split('_')[1] for k in sorted(data_set.axis_labels.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, key in enumerate(list_keys(data_set)):\n",
    "    print(\"%d %s\" % (i, key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Item = namedtuple('Item', ' '.join(list_keys(data_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "BLACK_LIST = string.punctuation.replace('%', '') + '\\n'\n",
    "\n",
    "def normalize(text_array, \n",
    "    black_list = BLACK_LIST, \n",
    "    vocab=None, lowercase =  True, tokenize = False):\n",
    "    text = text_array[0]\n",
    "    if black_list:\n",
    "        text = text.translate(None, BLACK_LIST)\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    if vocab:\n",
    "        text = ' '.join([word for word in text.split() if word in vocab])\n",
    "    if tokenize:\n",
    "        return text.split()\n",
    "    return text\n",
    "\n",
    "def encode_fast_text(label, text):\n",
    "    clean_label = label.lower().replace(\" \", \"-\")\n",
    "    clean_text = ' '.join(text[0][0].split())\n",
    "    return \"__label__%s %s\\n\" % (clean_label, clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dump_to_fastext_corpora(data_set, output, id2category,\n",
    "                            batch_size = BATH_SIZE, \n",
    "                            limit=None):\n",
    "    '''\n",
    "    Dumps the dataset to be consumed by fastText.\n",
    "    '''\n",
    "    N = data_set.num_examples\n",
    "    num_batch = N / batch_size\n",
    "    \n",
    "    handle = data_set.open()\n",
    "    with open(output, 'wr') as f:\n",
    "        processed = 0\n",
    "        for i in itertools.islice(xrange(num_batch), limit):\n",
    "            # fetch batch of data\n",
    "            text_batch, img_batch, metadata_batch  = data_set.get_data(\n",
    "                handle, slice(i*batch_size, min((i+1)*batch_size, N - processed)))\n",
    "            \n",
    "            # process batch\n",
    "            lines = [encode_fast_text(id2category[id_], text) for \n",
    "                     text, id_ in zip(text_batch, metadata_batch[:,0])]\n",
    "\n",
    "            # dumplines\n",
    "            f.writelines(lines)\n",
    "            \n",
    "            # track progress \n",
    "            processed += text_batch.shape[0]   \n",
    "            if i % 100 == 0:\n",
    "                percent = int(((100.0 * i )/ num_batch))\n",
    "                print(\"Processing %i batch out of %i [%i processed]\" % (i +1, num_batch +1, processed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dump_for_emmbeddings(\n",
    "    data_set, output, \n",
    "    batch_size = BATH_SIZE, limit=None, vocab=None):\n",
    "    '''\n",
    "    Dumps the hdf5 dataset to flat textfile\n",
    "    '''\n",
    "    N = data_set.num_examples\n",
    "    num_batch = N / batch_size + 1\n",
    "    \n",
    "    handle = data_set.open()\n",
    "    with open(output, 'wr') as f:\n",
    "        processed = 0\n",
    "        for i in itertools.islice(xrange(num_batch), limit):\n",
    "            # fetch batch of data\n",
    "            low, high = i*batch_size, min((i+1)*batch_size, data_set.num_examples)\n",
    "            rows = data_set.get_data(handle, slice(low, high))\n",
    "            \n",
    "            # process batch\n",
    "            classes = [row[0].replace(\" \", \"_\") for row in rows[1]]\n",
    "            texts = [normalize(text, vocab=vocab) for text in rows[4]]\n",
    "            lines = [\"%s %s\\n\" % (c, t) for c,t in zip(classes, texts)]\n",
    "            # dumplines\n",
    "            f.writelines(lines)\n",
    "             \n",
    "            if i % 100 == 0:\n",
    "                percent = int(((100.0 * i )/ num_batch))\n",
    "                print(\"Low: %d, high: %d\" % (low, high))\n",
    "                print(\"Processing %i batch out of %i [%i processed]\" % (i, num_batch, processed))\n",
    "            \n",
    "            # track progress \n",
    "            processed += len(texts)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "\n",
    "tokens = Counter()\n",
    "classes = Counter()\n",
    "\n",
    "handle = data_set.open()\n",
    "num_batch = data_set.num_examples / BATH_SIZE\n",
    "for i in xrange(num_batch):\n",
    "    if i % 100 == 0:\n",
    "        percent = int(((100.0 * i )/ num_batch))\n",
    "        print(\"Processing %i batch out of %i [%i percent]\" % (i, num_batch, percent))\n",
    "        print(\"Number of tokens in the dictionary: %i\" % len(tokens))\n",
    "        print(\"Number of classes in the dictionary: %i\" % len(classes))\n",
    "    \n",
    "    rows = data_set.get_data(\n",
    "        handle, \n",
    "        slice(i*BATH_SIZE, min((i+1)*BATH_SIZE, data_set.num_examples))\n",
    "    )\n",
    "    \n",
    "    tokens.update(itertools.chain(*[\n",
    "        normalize(text, tokenize=True) for text in rows[4]\n",
    "    ]))\n",
    "    classes.update(itertools.chain([\n",
    "        row[0] for row in rows[1]\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"There are %s distinct words in the dataset\" % len(tokens))\n",
    "print(\"There are %s distinct classes in the dataset\" % len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels, values = zip(*classes.most_common())\n",
    "\n",
    "indexes = np.arange(len(labels))\n",
    "width = 1\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.bar(indexes, values, width)\n",
    "plt.xticks(indexes + width * 0.5, labels, rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Are we missing something?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(LANGUAGE_MODEL_FILE, 'rt') as f:\n",
    "    vocab = set([line.split(\" \")[0] for line in f.readlines()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "missing_words = Counter({ word: c[word] for word in c if word.lower() not in vocab})\n",
    "print(\"There are %i missing words out of %i\" % (len(missing_words), len(c)))\n",
    "print(\"Most common mising words\")\n",
    "missing_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def clean_plot(img):\n",
    "    plt.imshow(img); plt.xticks([]); plt.yticks([]); plt.show()\n",
    "    \n",
    "def clean_plot_dpi(img, size, dpi=60):\n",
    "    plt.figure().set_size_inches(float(size)/float(dpi),float(size)/float(dpi))\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    plt.imshow(img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = random.randint(0, data_set.num_examples)\n",
    "item = Item._make(data_set.get_data(handle, slice(i, min((i+1), data_set.num_examples))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.title(item.category[0][0])\n",
    "plt.imshow(item.image[0]);\n",
    "print(\"Raw text: \\n%s\\n\" % item.description[0][0])\n",
    "print(\"Normalized text: \\n%s\\n\" %  ' '.join(\n",
    "    normalize(item.description[0], tokenize=True)))\n",
    "print(\"Normalized text with vocab: \\n%s\\n\" %  ' '.join(\n",
    "    normalize(item.description[0], vocab=vocab, tokenize=True)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be consistent with StackGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = item.image[0]\n",
    "img_76 = scipy.misc.imresize(img, [SMALL_SIZE, SMALL_SIZE], 'bicubic')\n",
    "img_304 = scipy.misc.imresize(img, [BIG_SIZE, BIG_SIZE], 'bicubic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_plot_dpi(img_76, SMALL_SIZE); clean_plot_dpi(img_304, BIG_SIZE);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text preprocessing for fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def clean_line(text):\n",
    "    return text.translate(None, BLACK_LIST).lower()\n",
    "\n",
    "def process(line):\n",
    "    label, text = line.split(\" \", 1)\n",
    "    return \"%s %s\\n\" % (label, clean_line(line))\n",
    "\n",
    "def strip(line):\n",
    "    _, text = line.split(\" \", 1)\n",
    "    return \"%s\\n\" % clean_line(text)\n",
    "\n",
    "with open(FASTTEXT_DATA, 'rt') as f:\n",
    "    all_lines = f.readlines()\n",
    "    lines = list(set(all_lines))\n",
    "\n",
    "with open(FASTTEXT_DATA_TRAIN, 'wt') as f:\n",
    "    f.writelines(sample([process(line) for line in lines], len(lines)))\n",
    "    \n",
    "with open(FASTTEXT_DATA_CLEAN, 'wt') as f:\n",
    "    f.writelines([strip(line) for line in all_lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump data for text embedding with CNNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dump_for_emmbeddings(data_set, OUTPUT, batch_size = BATH_SIZE, vocab=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
